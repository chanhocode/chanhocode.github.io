---
title: "[RAG] Embedding model ì‚¬ìš©í•´ë³´ê¸° with GeminiAPI & BGE-M3"
writer: chanho
date: 2024-11-28 21:28:00 +0900
categories: [AI, LargeLanguageModel]
tags: [ai, llm, rag]
---

# Embedding Model

ğŸ’¡ `ì„ë² ë”© ì´ë€?`

> ì„ë² ë”©ì€ í…ìŠ¤íŠ¸ì˜ ì‹œë§¨í‹± ì˜ë¯¸ì™€ ë§¥ë½ì„ í¬ì°©í•˜ì—¬ ìœ ì‚¬í•œ ì˜ë¯¸ë¥¼ ê°€ì§„ í…ìŠ¤íŠ¸ì˜ ì„ë² ë”©ì´ ë²¡í„° ê³µê°„ì—ì„œ ê°€ê¹ê²Œ ìœ„ì¹˜í•˜ë„ë¡ í•œë‹¤. ì˜ˆë¥¼ ë“¤ì–´ 'ê°•ì•„ì§€ë¥¼ ë™ë¬¼ ë³€ì›ì— ë°ë ¤ê°”ìŠµë‹ˆë‹¤"ì™€ "ê³ ì–‘ì´ë¥¼ ë™ë¬¼ë³‘ì›ì— ë°ë ¤ê°”ìŠµë‹ˆë‹¤" ë¼ëŠ” ë¬¸ì¥ì€ ìœ ì‚¬í•œ ë§¥ë½ì„ ê°€ì§€ë¯€ë¡œ, ì´ë“¤ì˜ ì„ë² ë”©ì€ ë²¡í„° ê³µê°„ì—ì„œ ì„œë¡œ ê°€ê¹ê²Œ ìœ„ì¹˜í•©ë‹ˆë‹¤.

## í™œìš© Documents

> í•´ë‹¹ ì„ë² ë”© ëª¨ë¸ í…ŒìŠ¤íŠ¸ë¥¼ ì§„í–‰í•˜ê¸° ìœ„í•œ ê°„ë‹¨í•œ ë°ì´í„°ë¥¼ ì¤€ë¹„í–ˆë‹¤.

```python
documents = [
  {
    "title": "ì»¨í…Œì´ë„ˆ ì„  _ íŒŒë‚˜ë§‰ìŠ¤ (Container Ship _ Panamax)",
    "content": "**íŒŒë‚˜ë§‰ìŠ¤(Panamax)**ì™€ **ë‰´ íŒŒë‚˜ë§‰ìŠ¤(New Panamax)**ëŠ” íŒŒë‚˜ë§ˆ ìš´í•˜ë¥¼ í†µê³¼í•  ìˆ˜ ìˆëŠ” ì„ ë°•ì˜ í¬ê¸° ì œí•œì„ ë‚˜íƒ€ë‚´ëŠ” ìš©ì–´ë¡œ, ìš´í•˜ì˜ êµ¬ì¡°ì  í•œê³„ì™€ ê·œê²©ì— ë”°ë¼ ì„¤ê³„ë˜ì—ˆìŠµë‹ˆë‹¤. ì´ ìš©ì–´ëŠ” ê¸°ì¡´ ìš´í•˜ì™€ í™•ì¥ëœ ìš´í•˜ë¥¼ ê¸°ì¤€ìœ¼ë¡œ êµ¬ë¶„ë©ë‹ˆë‹¤.\n\n**ì£¼ìš” íŠ¹ì§•:**\n- íŒŒë‚˜ë§‰ìŠ¤: ê¸°ì¡´ ìš´í•˜ ì„¤ê³„ì— ë§ì¶˜ ì„ ë°•ìœ¼ë¡œ, ìµœëŒ€ í˜ìˆ˜ 12.04m, ì ì¬ ìš©ëŸ‰ ì•½ 5,000 TEU.\n- ë‰´ íŒŒë‚˜ë§‰ìŠ¤: 2016ë…„ í™•ì¥ í›„ ë„ì…ëœ ê·œê²©ìœ¼ë¡œ, ë” í° ì„ ë°•ê³¼ ìµœëŒ€ 14,000 TEUê¹Œì§€ ì ì¬ ê°€ëŠ¥.\n\n**ì—­ì‚¬ì  ì˜ì˜:**\n- 1914ë…„ íŒŒë‚˜ë§ˆ ìš´í•˜ ê°œí†µìœ¼ë¡œ ê¸€ë¡œë²Œ ë¬¼ë¥˜ í˜ì‹ .\n- í¬ê¸° ì œí•œì€ íš¨ìœ¨ì  ë¬¼ë¥˜ì™€ ì„¤ê³„ í‘œì¤€í™”ë¥¼ ì´ëŒì–´ëƒ„.\n\n**ìš´í•­ íŠ¹ì§•:**\n- ìˆ˜ë¬¸ì˜ í­, ìˆ˜ë¡œ ê¹Šì´, ë‹¤ë¦¬ ë†’ì´ê°€ ì£¼ìš” ì œí•œ ìš”ì†Œ.\n- ì¡°ì¢…ì‚¬ íƒ‘ìŠ¹ ë° í™˜ê²½ ë³´í˜¸ ê·œì •ì„ ì¤€ìˆ˜í•´ì•¼ í•¨.\n\n**ë¯¸ë˜:** íŒŒë‚˜ë§ˆ ìš´í•˜ í™•ì¥ê³¼ ëŒ€í˜• ì„ ë°• ì¦ê°€ë¡œ ë¬¼ë¥˜ íš¨ìœ¨ì´ í–¥ìƒë˜ê³  í•´ìš´ ì—…ê³„ì˜ ë³€í™”ê°€ ì§€ì†ë  ì „ë§."
  },
  {
    "title": "ì»¨í…Œì´ë„ˆ ì„  _ ìˆ˜ì—ì¦ˆë§¥ìŠ¤ (Container Ship _ Suezmax)",
    "content": "**ìˆ˜ì—ì¦ˆë§¥ìŠ¤(Suezmax)**ëŠ” ìˆ˜ì—ì¦ˆ ìš´í•˜ë¥¼ í†µê³¼í•  ìˆ˜ ìˆëŠ” ìµœëŒ€ í¬ê¸°ì˜ ì„ ë°•ì„ ì •ì˜í•˜ë©°, ìš´í•˜ êµ¬ì¡°ì  ì œí•œ(ê¹Šì´, í­, ê¸¸ì´)ì„ ê¸°ë°˜ìœ¼ë¡œ ì„¤ê³„ë˜ì—ˆìŠµë‹ˆë‹¤.\n\n**ì£¼ìš” íŠ¹ì§•:**\n- ìµœëŒ€ í˜ìˆ˜ 20.1m, ìµœëŒ€ ì ì¬ ìš©ëŸ‰ ì•½ 10,000~15,000 TEU.\n- í™•ì¥ ê³µì‚¬ í›„ ë” í° ì„ ë°•(New Suezmax)ì˜ í†µê³¼ ê°€ëŠ¥ì„±.\n\n**ìš´í•­ íŠ¹ì§•:**\n- ìˆ˜ì‹¬ ì œí•œ: ì„ ë°• í˜ìˆ˜ëŠ” ìš´í•˜ ìˆ˜ë¡œì˜ ê¹Šì´ì— ë”°ë¼ ì œí•œ.\n- íš¨ìœ¨ì  ì„¤ê³„: ëŒ€ê·œëª¨ í™”ë¬¼ ìš´ì†¡ìœ¼ë¡œ ë¬¼ë¥˜ë¹„ ì ˆê°.\n\n**ì—­ì‚¬ì  ì˜ì˜:**\n- ì§€ì¤‘í•´ì™€ í™í•´ë¥¼ ì—°ê²°, ê¸€ë¡œë²Œ ë¬¼ë¥˜ì˜ 12%ë¥¼ ë‹´ë‹¹.\n- 2015ë…„ í™•ì¥ í›„ ìš©ëŸ‰ ì¦ê°€ì™€ ìš´ì†¡ ì‹œê°„ ë‹¨ì¶• ì‹¤í˜„.\n\n**ë¯¸ë˜:** ìš´í•˜ì˜ ì§€ì†ì  í™•ì¥ê³¼ ëŒ€í˜• ì„ ë°• ìˆ˜ìš” ì¦ê°€ë¡œ ê¸€ë¡œë²Œ í•´ìš´ ì—…ê³„ì˜ ì¤‘ìš”í•œ ìš”ì†Œë¡œ ìœ ì§€ë  ì „ë§."
  },
  {
    "title": "ì»¨í…Œì´ë„ˆ ì„  _ ì•„ë¼ë§‰ìŠ¤ (Container Ship _ Aramax)",
    "content": "**ì•„ë¼ë§‰ìŠ¤(Aramax)**ëŠ” ì•„ë¼ë¹„ì•„ë§Œ í•´ì—­ì„ ìš´í•­í•  ìˆ˜ ìˆëŠ” ìµœëŒ€ í¬ê¸° ì„ ë°•ì„ ì •ì˜í•˜ë©°, ìˆ˜ì‹¬ì´ ì–•ì€ í•´ì—­ê³¼ ì¢ì€ í•­ë§Œ ì¡°ê±´ì— ìµœì í™”ë˜ì–´ ìˆìŠµë‹ˆë‹¤.\n\n**ì£¼ìš” íŠ¹ì§•:**\n- ìµœëŒ€ í˜ìˆ˜ 15m, ì ì¬ ìš©ëŸ‰ ì•½ 6,000~8,000 TEU.\n- í˜¸ë¥´ë¬´ì¦ˆ í•´í˜‘ì„ ê¸°ì¤€ìœ¼ë¡œ ì„¤ê³„.\n\n**ìš´í•­ íŠ¹ì§•:**\n- ìµœì í™”ëœ ì„¤ê³„ë¡œ ì¤‘ë™ ì§€ì—­ì˜ ì—ë„ˆì§€ ìˆ˜ì†¡ ì§€ì›.\n- ì¢ì€ í•´ì—­ì—ì„œ ë†’ì€ ê¸°ë™ì„±ê³¼ ì•ˆì „ì„± ì œê³µ.\n\n**ì—­ì‚¬ì  ì˜ì˜:**\n- ì›ìœ  ë° LNG ìˆ˜ì¶œì„ íš¨ìœ¨ì ìœ¼ë¡œ ì§€ì›.\n- ì§€ì—­ í•­ë§Œê³¼ í•´í˜‘ ì œí•œì— ë§ì¶˜ ì„¤ê³„.\n\n**ë¯¸ë˜:** ì¤‘ë™ ì—ë„ˆì§€ ë¬¼ë¥˜ì˜ ì¤‘ìš”ì„± ì¦ê°€ì™€ í™˜ê²½ ê·œì œ ê°•í™”ì— ë”°ë¼ ì§€ì†ì ì¸ ë°œì „ê³¼ í˜„ëŒ€í™”ê°€ ê¸°ëŒ€ë¨."
  },
  {
    "title": "ë²Œí¬ì„  _ íŒŒë‚˜ë§‰ìŠ¤ (Bulk Carrier _ Panamax)",
    "content": "**íŒŒë‚˜ë§‰ìŠ¤(Panamax)** ë²Œí¬ì„ ì€ íŒŒë‚˜ë§ˆ ìš´í•˜ë¥¼ í†µê³¼í•  ìˆ˜ ìˆëŠ” í¬ê¸°ë¡œ ì„¤ê³„ëœ ë²Œí¬ í™”ë¬¼ ìš´ë°˜ì„ ì…ë‹ˆë‹¤.\n\n**ì£¼ìš” íŠ¹ì§•:**\n- ìµœëŒ€ í˜ìˆ˜ 12.04m, ì ì¬ ìš©ëŸ‰ ì•½ 65,000~75,000 í†¤.\n- ì„íƒ„, ê³¡ë¬¼, ì² ê´‘ì„ ë“± ë‹¤ì–‘í•œ í™”ë¬¼ ìš´ì†¡ ê°€ëŠ¥.\n\n**ìš´í•­ íŠ¹ì§•:**\n- í‘œì¤€í™”ëœ ì„¤ê³„ë¡œ ì „ ì„¸ê³„ í•­ë§Œ ì ‘ê·¼ì„± ë³´ìœ .\n- ë¬¼ë¥˜ íš¨ìœ¨ì„±ê³¼ ê²½ì œì„±ì„ ê·¹ëŒ€í™”.\n\n**ì—­ì‚¬ì  ì˜ì˜:**\n- íŒŒë‚˜ë§ˆ ìš´í•˜ë¥¼ ì¤‘ì‹¬ìœ¼ë¡œ í•œ ê¸€ë¡œë²Œ ë¬´ì—­ ë„¤íŠ¸ì›Œí¬ì˜ ì¤‘ì‹¬.\n- íŒŒë‚˜ë§ˆ ìš´í•˜ ì œí•œ ì¡°ê±´ì— ë§ì¶˜ ì„¤ê³„ë¡œ ë¬¼ë¥˜ë¹„ ì ˆê°.\n\n**ë¯¸ë˜:** ì§€ì†ì ì¸ ë²Œí¬ í™”ë¬¼ ìˆ˜ìš” ì¦ê°€ì™€ í™˜ê²½ ê·œì œ ê°•í™”ë¡œ íš¨ìœ¨ì ì´ê³  ì¹œí™˜ê²½ì ì¸ ì„¤ê³„ê°€ ë„ì…ë  ì „ë§."
  },
  {
    "title": "ë²Œí¬ì„  _ ì•„í”„ë¼ë§‰ìŠ¤ (Bulk Carrier _ Aframax)",
    "content": "**ì•„í”„ë¼ë§‰ìŠ¤(Aframax)** ë²Œí¬ì„ ì€ ì¤‘ëŒ€í˜• ë²Œí¬ í™”ë¬¼ ìš´ì†¡ì— ì‚¬ìš©ë˜ë©°, íš¨ìœ¨ì„±ê³¼ ê²½ì œì„±ì„ ì œê³µí•˜ëŠ” ì„ ë°•ì…ë‹ˆë‹¤.\n\n**ì£¼ìš” íŠ¹ì§•:**\n- ì ì¬ ìš©ëŸ‰ ì•½ 70,000~100,000 í†¤.\n- ì„íƒ„, ê³¡ë¬¼, ì² ê´‘ì„ ë“± ë‹¤ì–‘í•œ í™”ë¬¼ ìš´ì†¡ ê°€ëŠ¥.\n\n**ìš´í•­ íŠ¹ì§•:**\n- ì–•ì€ í•´ì—­ê³¼ ì¤‘í˜• í•­ë§Œì—ì„œì˜ ìš´í•­ ì í•©.\n- ê²½ì œì  ìš´ì†¡ìœ¼ë¡œ í™”ë¬¼ë‹¹ íƒ„ì†Œ ë°°ì¶œëŸ‰ ê°ì†Œ.\n\n**ì—­ì‚¬ì  ì˜ì˜:**\n- ì¤‘í˜• í•­ë§Œê³¼ í•´ì—­ì˜ ë¬¼ë¥˜ ìµœì í™”.\n- ì„ìœ  ë° ë²Œí¬ í™”ë¬¼ ìš´ì†¡ì˜ ì£¼ìš” ì„ ë°•ìœ¼ë¡œ í™œìš©.\n\n**ë¯¸ë˜:** ì§€ì† ê°€ëŠ¥ì„±ê³¼ ì¹œí™˜ê²½ ì„¤ê³„ë¡œ ê¸°ìˆ  í˜ì‹ ê³¼ ìƒˆë¡œìš´ ìš”êµ¬ë¥¼ ì¶©ì¡±í•  ê°€ëŠ¥ì„±ì´ í¼."
  },
  {
    "title": "ë²Œí¬ì„  _ ìˆ˜ì—ì¦ˆë§¥ìŠ¤ (Bulk Carrier _ Suezmax)",
    "content": "**ìˆ˜ì—ì¦ˆë§¥ìŠ¤(Suezmax)** ë²Œí¬ì„ ì€ ìˆ˜ì—ì¦ˆ ìš´í•˜ë¥¼ ê¸°ì¤€ìœ¼ë¡œ ì„¤ê³„ëœ ëŒ€í˜• ë²Œí¬ì„ ì…ë‹ˆë‹¤.\n\n**ì£¼ìš” íŠ¹ì§•:**\n- ìµœëŒ€ í˜ìˆ˜ 20.1m, ì ì¬ ìš©ëŸ‰ ì•½ 150,000~160,000 í†¤.\n- ëŒ€ê·œëª¨ í™”ë¬¼ ìš´ë°˜ìœ¼ë¡œ ê²½ì œì„± ê·¹ëŒ€í™”.\n\n**ìš´í•­ íŠ¹ì§•:**\n- ìˆ˜ë¡œ ê¹Šì´ì™€ í­ ì œí•œ ì¤€ìˆ˜ë¡œ ì„¤ê³„.\n- ì„íƒ„, ì² ê´‘ì„, ê³¡ë¬¼ ë“± ëŒ€ëŸ‰ í™”ë¬¼ ìš´ì†¡.\n\n**ì—­ì‚¬ì  ì˜ì˜:**\n- ìˆ˜ì—ì¦ˆ ìš´í•˜ë¥¼ í†µí•œ ê¸€ë¡œë²Œ ë¬¼ë¥˜ íë¦„ì˜ ì¤‘ì‹¬.\n- ê²½ì œì  ì´ì ê³¼ í™˜ê²½ì  ì´ì ì„ ë™ì‹œì— ì œê³µ.\n\n**ë¯¸ë˜:** ëŒ€í˜• ë²Œí¬ í™”ë¬¼ ìˆ˜ìš”ì™€ í™˜ê²½ ê·œì œ ê°•í™”ì— ë”°ë¥¸ ì„¤ê³„ ë° ê¸°ìˆ  í˜ì‹ ì´ ê¸°ëŒ€ë¨."
  }
]

df = pd.DataFrame(documents)
df.columns = ['title', 'content']
```

## Gemini API Embedding

> RAGì˜ í…ŒìŠ¤íŠ¸ë¥¼ ì§„í–‰í•˜ê¸° ìœ„í•´ ê°„ë‹¨í•˜ê²Œ ì‚¬ìš©í•  ìˆ˜ ìˆëŠ” Googleì˜ Gemini API Embeddingì— ëŒ€í•´ ì•Œì•„ë³´ê³  ì‚¬ìš©í•´ë³´ê³ ì í•œë‹¤.

`Google`ì˜ Gemini APIëŠ” í…ìŠ¤íŠ¸ ì„ë² ë”©ì„ ìƒì„±í•˜ëŠ” ëª¨ë¸ì„ ì œê³µì¤‘ì´ë‹¤. í•´ë‹¹ ì„ë² ë”©ì€ í…ìŠ¤íŠ¸ì˜ ì˜ë¯¸ì™€ ë§¥ë½ì„ ìˆ˜ì¹˜ ë²¡í„°ë¡œ í‘œí˜„í•˜ì—¬ ì‹œë©˜í‹± ê²€ìƒ‰, í…ìŠ¤íŠ¸ ë¶„ë¥˜, í´ëŸ¬ìŠ¤í„°ë§ ë“± ë‹¤ì–‘í•œ AI ì‘ì—…ì— ì‘ìš©í•  ìˆ˜ ìˆë‹¤.

### Gemni Embdeeing Model

> Geminiì—ì„œ APIë¡œ ì œê³µë˜ëŠ” ì„ë² ë”© ëª¨ë¸ì€ ì•„ë˜ ì½”ë“œë¥¼ í†µí•´ í™•ì¸ì´ ê°€ëŠ¥í•˜ë‹¤.

```python
for model in genai.list_models():
    if "embedContent" in model.supported_generation_methods:
        print(model.name)
```

: í•´ë‹¹ ì½”ë“œì˜ ê²°ê³¼ë¡œ í˜„ì¬ ë‘ê°œì˜ ì„ë² ë”© ëª¨ë¸ì„ í™•ì¸í•  ìˆ˜ ìˆë‹¤.

- models/embedding-001
- models/text-embedding-004

### Document Embedding

```python
def embed_fn(title, text):
    return genai.embed_content(
        model="models/text-embedding-004", content=text, task_type="retrieval_document", title=title
    )

df["Embeddings"] = df.apply(lambda row: embed_fn(row["title"], row["content"]), axis=1)
```

: í•´ë‹¹ ì½”ë“œë¥¼ í†µí•´ Documentë¥¼ 'text-embedding-004'ë¥¼ í†µí•´ ì„ë² ë”© í•˜ì˜€ë‹¤.

```markdown
- ë°ì´í„°í”„ë ˆì„ ì°¨ì›: (6, 3)
- ë°ì´í„°í”„ë ˆì„ ì»¬ëŸ¼: Index(['title', 'content', 'Embeddings'], dtype='object')

# Embeddingsì˜ ì²« ë²ˆì§¸ ë°ì´í„°ì˜ ì„ë² ë”© ê²°ê³¼ì˜ ì¼ë¶€

> {'embedding': [-0.01840346, 0.015210435, 0.019861942, 0.016452577, 0.045706112, -0.027615234, 0.053394984, -0.03504379, 0.049246717, 0.039555907, -0.0156078795, 0.03842172, 0.06263071, -0.0039767427, 0.033486232, -0.028591583, 0.01640767, -0.045544785, -0.07141772, -0.0105059175, -0.015567026, 0.022975547, 0.06594105, -0.041251298, 0.0041016345, -0.015563263, -0.020574883, 0.045865867, 0.009999366, -0.00858956, 0.063167036, 0.040786996, 0.017781593, -0.032296445, 0.03935606, -0.003304071, -0.00806855, 0.016891267, 0.053529307, -0.08014855, -0...]}
```

## Question Embedding ë° ìœ ì‚¬ë„ ë†’ì€ ë¬¸ì„œ ì¶œë ¥

> ìœ„ ì—ì„œ 'text-embedding-004'ë¡œ documentsë¥¼ ì„ë² ë”© í–ˆìœ¼ë¯€ë¡œ ì§ˆë¬¸ì„ ì‘ì„±í•˜ê³  ë™ì¼í•œ ëª¨ë¸ë¡œ ì„ë² ë”©ì„ í•´ì„œ ì ê³± ê³„ì‚°ì„ í†µí•´ ì§ˆë¬¸ê³¼ ë¬¸ì„œì˜ ìœ ì‚¬ë„ë¥¼ êµ¬í•  ìˆ˜ ìˆë‹¤. ì ê³±ì€ ë‘ ë²¡í„° ê°„ì˜ ë°©í–¥ì„±ê³¼ í¬ê¸°ë¥¼ ë™ì‹œì— ê³ ë ¤í•˜ì—¬ ìœ ì‚¬ë„ ì ìˆ˜ë¥¼ ì œê³µí•˜ë©° í¬ê¸°ê°€ ë™ì¼í•˜ê³  ë°©í–¥ì´ ìœ ì‚¬í• ìˆ˜ë¡ ì ê³² ê°’ì´ ì»¤ì§€ë©°, ì´ëŠ” ë‘ ë²¡í„°ê°€ ì˜ë¯¸ì ìœ¼ë¡œ ê°€ê¹Œì›€ì„ ë‚˜íƒ€ë‚¸ë‹¤.

- ì§ˆë¬¸: `ì»¨í…Œì´ë„ˆ ì„  ì¤‘ì—ì„œ ìµœëŒ€ í˜ìˆ˜ 12.04mì¸ ì„ ë°•ì€ ì–´ë–¤ ì¢…ë¥˜ ì¸ê°€?`
- ëª¨ë¸: `models/text-embedding-004`

```python
def find_best_passage(query, dataframe):
    """
    ì¿¼ë¦¬ì™€ ë°ì´í„°í”„ë ˆì„ ë‚´ì˜ ê° ë¬¸ì„œ ì‚¬ì´ì˜ ê±°ë¦¬ë¥¼ ì ê³±ì„ ì´ìš©í•˜ì—¬ ê³„ì‚°í•©ë‹ˆë‹¤.
    """

    query_embedding = genai.embed_content(model="models/text-embedding-004", content=query, task_type="retrieval_query")
    query_vec = query_embedding.get("embedding")

    if query_vec is None:
        raise ValueError("Query embedding is missing or malformed")
    print("Query embedding generated:", query_vec[:5])

    embeddings = np.stack(dataframe["Embeddings"].apply(lambda x: x["embedding"]))
    print("Embeddings shape:", embeddings.shape)
    print("dataframe:", len(embeddings))

    if embeddings.shape[1] != len(query_vec):
        raise ValueError("Embedding dimensions do not match")

    # ì ê³± ê³„ì‚°
    dot_products = np.dot(embeddings, query_vec)
    print("Dot products:", dot_products)

    # ê°€ì¥ ë†’ì€ ì ìˆ˜ë¥¼ ê°€ì§„ í…ìŠ¤íŠ¸ ë°˜í™˜
    idx = np.argmax(dot_products)
    return dataframe.iloc[idx]

query = "ì»¨í…Œì´ë„ˆ ì„  ì¤‘ì—ì„œ ìµœëŒ€ í˜ìˆ˜ 12.04mì¸ ì„ ë°•ì€ ì–´ë–¤ ì¢…ë¥˜ ì¸ê°€?"
model = "models/text-embedding-004"

passage = find_best_passage(query, df)
```

ìœ„ ì½”ë“œë¥¼ ìˆ˜í–‰í•˜ì—¬ ì§ˆë¬¸ê³¼ ë¬¸ì„œê°„ ìœ ì‚¬ë„ë¥¼ ì¸¡ì •í•˜ê³  ê°€ì¥ ë†’ì€ ìœ ì‚¬ë„ë¥¼ ê°€ì§„ ë¬¸ì„œë¥¼ ê°€ì ¸ì˜¬ ìˆ˜ ìˆë‹¤.

```
Query: ì»¨í…Œì´ë„ˆ ì„  ì¤‘ì—ì„œ ìµœëŒ€ í˜ìˆ˜ 12.04mì¸ ì„ ë°•ì€ ì–´ë–¤ ì¢…ë¥˜ ì¸ê°€?
Query embedding generated: [-0.001457668, 0.011586195, -0.058092363, 0.049184415, 0.062245063]
Embeddings shape: (6, 768)
dataframe: 6
Dot products: [0.5815024  0.54061835 0.49384623 0.6115153  0.49807219 0.55653029]

title                       ë²Œí¬ì„  _ íŒŒë‚˜ë§‰ìŠ¤ (Bulk Carrier _ Panamax)
content       **íŒŒë‚˜ë§‰ìŠ¤(Panamax)** ë²Œí¬ì„ ì€ íŒŒë‚˜ë§ˆ ìš´í•˜ë¥¼ í†µê³¼í•  ìˆ˜ ìˆëŠ” í¬ê¸°ë¡œ ì„¤ê³„...
Embeddings    {'embedding': [0.018026227, -0.0034669237, 0.0...
Name: 3, dtype: object
```

ìˆ˜í–‰í•œ ê²°ê³¼ëŠ” ìœ„ì—ì™€ ê°™ë‹¤. ê²°ê³¼ë¥¼ ë³´ë©´ ìµœëŒ€ í˜ìˆ˜ 12.04m ì¸ ì‚¬ì´ì¦ˆëŠ” íŒŒë‚˜ë§‰ìŠ¤ë¥¼ ì˜ ì°¾ì•˜ì§€ë§Œ ì§ˆë¬¸ì— ì»¨í…Œì´ë„ˆ ì„  ì´ë¼ê³  í–ˆì§€ë§Œ, ë²Œí¬ì„  íŒŒë‚˜ë§‰ìŠ¤ë¥¼ ìœ ì‚¬ë„ê°€ ê°€ì¥ ë†’ê²Œ ë‚˜ì™”ë‹¤. ì´ì— í•œê¸€ ë§ê³  ì˜ì–´ë¡œ `What type of container ship has a maximum draft of 12.04m?` ì´ë¼ê³  ì§ˆë¬¸ í•œ ê²°ê³¼ëŠ” ì•„ë˜ì™€ ê°™ë‹¤.

```
title                  ì»¨í…Œì´ë„ˆ ì„  _ íŒŒë‚˜ë§‰ìŠ¤ (Container Ship _ Panamax)
content       **íŒŒë‚˜ë§‰ìŠ¤(Panamax)**ì™€ **ë‰´ íŒŒë‚˜ë§‰ìŠ¤(New Panamax)**ëŠ” íŒŒë‚˜...
Embeddings    {'embedding': [-0.01840346, 0.015210435, 0.019...
Name: 0, dtype: object
```

ì§ˆë¬¸ì—ì„œ ì˜ë„í•œ 'ì»¨í…Œì´ë„ˆì„  íŒŒë‚˜ë§‰ìŠ¤'ì— ëŒ€í•œ ë¬¸ì„œë¥¼ ì œëŒ€ë¡œ ê°€ì ¸ ì™”ë‹¤. ì´ì— Googleì˜ 'text-embedding-004'ëŠ” í•œêµ­ì–´ë¥¼ ì§€ì›í•œë‹¤ê³  ê³µì‹ ë¬¸ì„œì— ì í˜€ìˆì§€ë§Œ, ë¯¸ì„¸ì¡°ì • ì—†ì´ ì„œë¹„ìŠ¤ì— ì´ìš©í•˜ê¸°ì—ëŠ” ì£¼ì˜ê°€ í•„ìš”í•  ê²ƒìœ¼ë¡œ ë³´ì¸ë‹¤. ì´ì— ì°¾ì•„ë³¸ ë°”ë¡œëŠ” Googleì—ì„œ ë¯¸ì„¸ì¡°ì •ì„ ì§€ì›í•˜ëŠ” ëª¨ë¸ì€ `textembedding-gecko` ë° `textembedding-gecko-multilingual` ì—ì„œ ì§€ì› í•˜ëŠ” ê²ƒìœ¼ë¡œ ë³´ì´ë¯€ë¡œ ì¢€ ë” ì•Œì•„ë³´ë ¤ë©´ [í•´ë‹¹ ë§í¬](https://cloud.google.com/vertex-ai/generative-ai/docs/models/tune-embeddings?hl=ko)ë¥¼ ì°¸ì¡°í•˜ë©´ ë„ì›€ì´ ë  ê²ƒìœ¼ë¡œ ë³´ì¸ë‹¤.

`ì´ì— 'Huggingface'ì—ì„œ 'Open weight'ëª¨ë¸ë¡œ ê³µê°œ ë˜ì–´ ìˆìœ¼ë©´ì„œ í•œêµ­ì–´ ì„ë² ë”©ì—ë„ ì¢‹ì€ ì„±ëŠ¥ì„ ë³´ì¸ë‹¤ëŠ” BGE-m3ì™€ ê°™ì€ ëª¨ë¸ì„ ì¢€ ë” ì•Œì•„ë³´ê³ ì í•œë‹¤.`

## dragonkue/BGE-m3-ko Local Model Embedding

> dragonkueì—ì„œ 'BAAI/bge-m3'ëª¨ë¸ì„ í•œê¸€ ë°ì´í„°ì…‹ì„ ì´ìš©í•´ íŒŒì¸íŠœë‹í•œ ì„ë² ë”© ëª¨ë¸ë¡œ bge-m3 ëª¨ë¸ì—ëŠ” ì˜ì–´ì™€ ì¤‘êµ­ì–´ ì´ì™¸ì— ë‹¤ë¥¸ ì–¸ì–´ì—ëŠ” í•™ìŠµì´ ë¶€ì¡±í•˜ê¸°ì— 'Ai hub ê¸°ê³„ë…í•´' ë°ì´í„°ì…‹ì„ í™œìš©í•´ ì¶”ê°€ í•™ìŠµí•œ ëª¨ë¸ì´ë‹¤. í˜„ì¬ RAGë¥¼ í™œìš©í•  ë°ì´í„°ì…‹ì´ ë§¤ìš° ì ê¸° ë•Œë¬¸ì— ë¯¸ë¦¬ í•œêµ­ì–´ íŒŒì¸íŠœë‹ëœ ëª¨ë¸ì„ í™œìš©í•˜ê³  ê¸°íšŒê°€ ë˜ë©´ í•´ë‹¹ ëª¨ë¸ì— ì¶”ê°€ í•™ìŠµì„ ì§„í–‰í•˜ì—¬ ì„ì‹œì ìœ¼ë¡œ ì‚¬ìš©í•´ë³´ë ¤ê³  ê³„íšì¤‘ ì´ë‹¤.

### SentenceTransformer í™œìš© ëª¨ë¸ ì‚¬ìš©

> Transformersì—ì„œ ì§€ì›í•˜ëŠ” SentenceTransformerë¥¼ í™œìš©í•˜ë©´ ëª¨ë¸ì„ ì§ì ‘ ë‹¤ìš´í•˜ì§€ ì•Šê³ ë„ í•´ë‹¹ ëª¨ë¸ì„ ë¡œë“œí•´ì„œ ì‚¬ìš©í•´ ë³¼ ìˆ˜ ìˆë‹¤. ì‚¬ìš©í•˜ê³ ì í•˜ëŠ” 'BGE-m3-ko' ëª¨ë¸ì˜ ê³µì‹ ë¬¸ì„œì˜ ì˜ˆì œ ì½”ë“œëŠ” ì•„ë˜ì™€ ê°™ë‹¤.

```python
from sentence_transformers import SentenceTransformer

# Download from the ğŸ¤— Hub
model = SentenceTransformer("dragonkue/bge-m3-ko")
# Run inference
sentences = [
    'ìˆ˜ê¸‰ê¶Œì ì¤‘ ê·¼ë¡œ ëŠ¥ë ¥ì´ ì—†ëŠ” ì„ì‚°ë¶€ëŠ” ëª‡ ì¢…ì— í•´ë‹¹í•˜ë‹ˆ?',
    'ë‚´ë…„ë¶€í„° ì €ì†Œë“ì¸µ 1ì„¸ ë¯¸ë§Œ ì•„ë™ì˜ \nì˜ë£Œë¹„ ë¶€ë‹´ì´ ë” ë‚®ì•„ì§„ë‹¤!\nì˜ë£Œê¸‰ì—¬ì œë„ ê°œìš”\nâ–¡ (ëª©ì ) ìƒí™œìœ ì§€ ëŠ¥ë ¥ì´ ì—†ê±°ë‚˜ ìƒí™œì´ ì–´ë ¤ìš´ êµ­ë¯¼ë“¤ì—ê²Œ ë°œìƒí•˜ëŠ” ì§ˆë³‘, ë¶€ìƒ, ì¶œì‚° ë“±ì— ëŒ€í•´ êµ­ê°€ê°€ ì˜ë£Œì„œë¹„ìŠ¤ ì œê³µ\nâ–¡ (ì§€ì›ëŒ€ìƒ) êµ­ë¯¼ê¸°ì´ˆìƒí™œë³´ì¥ ìˆ˜ê¸‰ê¶Œì, íƒ€ ë²•ì— ì˜í•œ ìˆ˜ê¸‰ê¶Œì ë“±\n\n| êµ¬ë¶„ | êµ­ë¯¼ê¸°ì´ˆìƒí™œë³´ì¥ë²•ì— ì˜í•œ ìˆ˜ê¸‰ê¶Œì | êµ­ë¯¼ê¸°ì´ˆìƒí™œë³´ì¥ë²• ì´ì™¸ì˜ íƒ€ ë²•ì— ì˜í•œ ìˆ˜ê¸‰ê¶Œì |\n| --- | --- | --- |\n| 1ì¢… | â—‹ êµ­ë¯¼ê¸°ì´ˆìƒí™œë³´ì¥ ìˆ˜ê¸‰ê¶Œì ì¤‘ ê·¼ë¡œëŠ¥ë ¥ì´ ì—†ëŠ” ìë§Œìœ¼ë¡œ êµ¬ì„±ëœ ê°€êµ¬ - 18ì„¸ ë¯¸ë§Œ, 65ì„¸ ì´ìƒ - 4ê¸‰ ì´ë‚´ ì¥ì• ì¸ - ì„ì‚°ë¶€, ë³‘ì—­ì˜ë¬´ì´í–‰ì ë“± | â—‹ ì´ì¬ë¯¼(ì¬í•´êµ¬í˜¸ë²•) â—‹ ì˜ìƒì ë° ì˜ì‚¬ìì˜ ìœ ì¡±â—‹ êµ­ë‚´ ì…ì–‘ëœ 18ì„¸ ë¯¸ë§Œ ì•„ë™â—‹ êµ­ê°€ìœ ê³µì ë° ê·¸ ìœ ì¡±â€¤ê°€ì¡±â—‹ êµ­ê°€ë¬´í˜•ë¬¸í™”ì¬ ë³´ìœ ì ë° ê·¸ ê°€ì¡±â—‹ ìƒˆí„°ë¯¼(ë¶í•œì´íƒˆì£¼ë¯¼)ê³¼ ê·¸ ê°€ì¡±â—‹ 5â€¤18 ë¯¼ì£¼í™”ìš´ë™ ê´€ë ¨ì ë° ê·¸ ìœ ê°€ì¡±â—‹ ë…¸ìˆ™ì¸ â€» í–‰ë ¤í™˜ì (ì˜ë£Œê¸‰ì—¬ë²• ì‹œí–‰ë ¹) |\n| 2ì¢… | â—‹ êµ­ë¯¼ê¸°ì´ˆìƒí™œë³´ì¥ ìˆ˜ê¸‰ê¶Œì ì¤‘ ê·¼ë¡œëŠ¥ë ¥ì´ ìˆëŠ” ê°€êµ¬ | - |\n',
    'ì´ì–´ ì´ë‚  ì˜¤í›„ 1ì‹œ30ë¶„ë¶€í„° ì—´ë¦´ ì˜ˆì •ì´ë˜ ìŠ¤ë…¸ë³´ë“œ ì—¬ì ìŠ¬ë¡œí”„ìŠ¤íƒ€ì¼ ì˜ˆì„  ê²½ê¸°ëŠ” ì—°ê¸°ë¥¼ ê±°ë“­í•˜ë‹¤ ì·¨ì†Œëë‹¤. ì¡°ì§ìœ„ëŠ” ì˜ˆì„  ì—†ì´ ë‹¤ìŒ ë‚  ê²°ì„ ì—ì„œ ì°¸ê°€ì 27ëª…ì´ í•œë²ˆì— ê²½ê¸°í•´ ìˆœìœ„ë¥¼ ê°€ë¦¬ê¸°ë¡œ í–ˆë‹¤.',
]
embeddings = model.encode(sentences)
print(embeddings.shape)
# [3, 1024]

# Get the similarity scores for the embeddings
similarities = model.similarity(embeddings, embeddings)
print(similarities.shape)
# [3, 3]
```

> í•´ë‹¹ ë°©ë²•ì„ í™œìš©í•˜ë©´ ëª¨ë¸ì„ ë¡œë“œí•˜ê³  ì‚¬ìš©í•´ ë³¼ ìˆ˜ ìˆë‹¤.

### transformersì˜ AutoModel í™œìš© Localì—ì„œ Embedding ëª¨ë¸ ì‚¬ìš©

> ë‚˜ëŠ” ëª¨ë¸ì„ ë¡œì»¬ì— ì €ì¥í•˜ê³  ì‚¬ìš©ì„ í•˜ê³  ì‹¶ì–´ í•´ë‹¹ ëª¨ë¸ì„ ë¡œì»¬ì— ë°›ì•„ì„œ ìœ„ ì½”ë“œì— ëª¨ë¸ ê²½ë¡œë¥¼ ë§¤ê°œë³€ìˆ˜ë¡œ ì£¼ì—ˆë”ë‹ˆ value errorê°€ ë°œìƒí•˜ì˜€ë‹¤. ë˜í•œ langchainì˜ HuggingFaceEmbeddingsì„ ì´ìš©í•´ë„ ë™ì¼í•œ ë¬¸ì œê°€ ë°œìƒí•˜ì˜€ë‹¤. ë‚´ê°€ ì •í™•í•œ ë°©ë²•ì„ ì°¾ì§€ ëª»í•œ ê±¸ ìˆ˜ë„ ìˆì§€ë§Œ, ë‹¤ë¥¸ ë°©ë²•ì„ ì°¾ì•˜ë‹¤ ì´ë¥¼ ê³µìœ í•˜ê³ ì í•œë‹¤.

```python
def encode_sentences(sentences, model_path):
    """
    ë¬¸ì¥ ë¦¬ìŠ¤íŠ¸ë¥¼ ì…ë ¥ìœ¼ë¡œ ë°›ì•„ í•´ë‹¹ ë¬¸ì¥ì˜ ì„ë² ë”© ë²¡í„°ë¥¼ ë°˜í™˜í•˜ëŠ” í•¨ìˆ˜.

    Args:
    - sentences (list of str): ì„ë² ë”©í•  ë¬¸ì¥ ë¦¬ìŠ¤íŠ¸.
    - model_path (str): ë¡œì»¬ ë˜ëŠ” í—ˆê¹…í˜ì´ìŠ¤ ëª¨ë¸ ê²½ë¡œ.

    Returns:
    - torch.Tensor: ë¬¸ì¥ ì„ë² ë”© í…ì„œ. í¬ê¸°ëŠ” (num_sentences, embedding_dim).
    """
    tokenizer = AutoTokenizer.from_pretrained(model_path)
    model = AutoModel.from_pretrained(model_path)

    inputs = tokenizer(sentences, padding=True, truncation=True, return_tensors="pt")

    with torch.no_grad():  # í•™ìŠµ ì¤‘ì´ ì•„ë‹ˆë¯€ë¡œ gradient ê³„ì‚° ë°©ì§€
        outputs = model(**inputs)

    # Pooling (mean pooling)
    embeddings = torch.mean(outputs.last_hidden_state, dim=1)
    return embeddings

query_embedding = encode_sentences([query], model_path)[0]
print("Query embedding shape:", query_embedding.shape)
```

: í•´ë‹¹ í•¨ìˆ˜ë¥¼ ì‚¬ìš©í•´ ì§ˆë¬¸ê³¼ ë¬¸ì„œë“¤ì„ ì„ë² ë”© í•  ìˆ˜ ìˆë‹¤. 'text-embedding-004' ëª¨ë¸ê³¼ ë¹„êµë¥¼ ìœ„í•´ ë™ì¼í•œ ë¬¸ì„œì™€ ì§ˆë¬¸ì„ ì´ìš©í•´ì„œ ê²€ìƒ‰ì„ ìˆ˜í–‰í•œ ê²°ê³¼ëŠ” ì•„ë˜ì™€ ê°™ë‹¤.

```
ì§ˆë¬¸: "ì»¨í…Œì´ë„ˆ ì„  ì¤‘ì—ì„œ ìµœëŒ€ í˜ìˆ˜ 12.04mì¸ ì„ ë°•ì€ ì–´ë–¤ ì¢…ë¥˜ ì¸ê°€?"

Query embedding shape: torch.Size([1024])
Embeddings shape: torch.Size([6, 1024])
Best similarity score: 0.7852264046669006
Best Passage:
title               ì»¨í…Œì´ë„ˆ ì„  _ íŒŒë‚˜ë§‰ìŠ¤ (Container Ship _ Panamax)
content    **íŒŒë‚˜ë§‰ìŠ¤(Panamax)**ì™€ **ë‰´ íŒŒë‚˜ë§‰ìŠ¤(New Panamax)**ëŠ” íŒŒë‚˜...
Name: 0, dtype: object

---

ì§ˆë¬¸2: "What type of container ship has a maximum draft of 12.04m?"

Query embedding shape: torch.Size([1024])
Embeddings shape: torch.Size([6, 1024])
Best similarity score: 0.7567333579063416
Best Passage:
title               ì»¨í…Œì´ë„ˆ ì„  _ íŒŒë‚˜ë§‰ìŠ¤ (Container Ship _ Panamax)
content    **íŒŒë‚˜ë§‰ìŠ¤(Panamax)**ì™€ **ë‰´ íŒŒë‚˜ë§‰ìŠ¤(New Panamax)**ëŠ” íŒŒë‚˜...
Name: 0, dtype: object
```

> ìœ„ ê²°ê³¼ì™€ ê°™ì´ í•œê¸€ê³¼ ì˜ì–´ ë‘ ë¶€ë¶„ ëª¨ë‘ ë‚´ê°€ ì›í•˜ëŠ” ê²°ê³¼ë¥¼ ì£¼ì—ˆë‹¤. í•´ë‹¹ ëª¨ë¸ì„ ì´ìš©í•´ì„œ ì¢€ ë” í° ë°ì´í„°ë¥¼ ë‹¤ë¤„ë³´ê³  ê²°ê³¼ê°€ ì¢‹ìœ¼ë©´ ì¢€ë” íŒŒì¸íŠœë‹ì„ í†µí•´ ì„œë¹„ìŠ¤ì— ì í•©í•œ ëª¨ë¸ë¡œ í™œìš©í•´ ë³¼ ìˆ˜ ìˆì„ ê²ƒìœ¼ë¡œ ë³´ì¸ë‹¤.

## ê²€í†  ì˜ˆì • í•œêµ­ì–´ ì„ë² ë”© ëª¨ë¸ with Hugging Face

- [bespin-global/klue-sroberta-base-continue-learning-by-mnr](https://huggingface.co/bespin-global/klue-sroberta-base-continue-learning-by-mnr)
- [intfloat/multilingual-e5-large](https://huggingface.co/intfloat/multilingual-e5-large)
