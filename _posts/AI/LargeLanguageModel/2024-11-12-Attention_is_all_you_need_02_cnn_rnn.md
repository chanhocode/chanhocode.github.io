---
title: "[LMM] Attention is All You Need를 읽어보자(2)_CNN 과 RNN에 대해서"
writer: chanho
date: 2024-11-12 22:11:00 +0900
categories: [AI, LargeLanguageModel]
tags: [ai, llm]
---

> 어제 간단하게 Attention is All You Need를 읽으며 Intro에서 사전 지식으로 간단하게 알면 좋을 거 같은 몇몇개의 키워드를 추렸다. 그중에서 오늘은 'RNN'과 'CNN'에 대해 간단하게 살펴보고자 한다.

# CNN(Convolutional Neural Networks)

> CNN은 인간의 시신경 구조를 모방한 이미지 및 영상 인식을 위한 딥러닝의 기본 모델이다. 특히 컴퓨터 비전 분야에서 강력한 성능을 발휘하며, 이미지 분류, 객체 탐지, 이미지 생성 등 다양한 작업에 활용된다.

이미지의 Raw Input을 그대로 받으므로 공간적/지역적 정보를 유지한채 Feature들의 계층을 빌드업 한다.

![Typical_cnn](https://github.com/user-attachments/assets/209f3753-a477-4452-af4f-0dddcf1d7ce3)
_Image Source: [wekipedia_cnn](https://en.wikipedia.org/wiki/Convolutional_neural_network)_

- `Convolutional Layer`: 합성곱 연산을 수행하여 신경망의 복잡도를 낮추는 층
- `Pulling Layer`: Pulling 연산을 수행하여 이미지의 패턴을 추출하는 층
- `Fully Connected Layer`: MLP와 같은 구조의 최종 출력 층

## 주요 특징

- 컨볼루션 연산: CNN은 이미지 각 작은 부분(로컬 영역)을 필터(또는 커널)와 곱해 특정한 특징을 추출하는 곱셈 연산을 수행한다. 이 필터는 이미지 내의 경계, 질감, 패턴 같은 유용한 특징들을 감지하는 데 사용된다.
- 계층 구조: CNN은 여러 개의 컨볼루션 계층을 쌓아, 계층이 깊어질수록 더 복잡한 패턴과 고차원적 특징을 학습한다. 초기 계층은 간단한 형태나 색상 같은 저차원적 특징을, 깊은 계층은 더욱 복잡한 모양이나 객체 같은 고차원적 특징을 인식할 수 있다.
- 파라미터 공유: 이미지 내의 동일한 특징을 여러 위치에서 찾기 위해 동일한 필터를 반복 사용하므로, 다른 신경망에 비해 학습해야 하는 파라미터가 적어 메모리와 연산 효율이 높습니다.
- MLP의 개선: MLP(Multi-Layer Perception: 다층 퍼셉트론)는 여러개의 퍼셉트론 뉴런을 다층으로 쌓은 구조이다. 인접한 두층의 뉴런 간에는 'Full-connected Layer(완전 연결층)'을 구성하며, 은닉층을 추가 할 수록 파라미터 수가 많아지고 복잡도가 높아진다. CNN은 인접한 두 층간의 뉴런들이 완전 연결층을 이루지 않으므로 MLP를 개선한다.

## Convolutional

![Convolutional_neural_network,_convolution_worked_example](https://github.com/user-attachments/assets/508f3b1b-9bad-49f4-b1d1-2411b0523b50)
_Image Source: [wekipedia_cnn](https://en.wikipedia.org/wiki/Convolutional_neural_network)_

> 입력 데이터는 Filter 행렬과 합성곱을 수행하며 이미지 차원이 축소되고(Padding을 안한다면) Featrue Map이 생성된다.

## Polling

> 데이터의 공간 크기를 줄여주는 연산으로, 특징 맵(Feature map)의 중요한 정보를 유지하면서도 연산량을 줄이고 과적합(Overfitting)을 방지하는 데 사용된다.

- 차원 축소: 입력 데이터의 크기를 줄여 CNN 모델의 계산 효율을 높인다.
- 중요 정보 유지: Pooling은 각 작은 영역에서 가장 중요한 정보를 추출하여 특징 맵의 중요한 패턴을 유지한다.
- 위치 불변성: 이미지의 작은 위치 변화에 대한 불변성을 강화할 수 있어, 객체가 조금 움직이거나 크기가 변하더라도 인식 성능이 유지된다.

- `Max Pooling`: 각 작은 영역에서 가장 큰 값을 선택하여 다음 계층으로 전달한다. 이를 통해 중요한 특징을 강조하는 효과가 발생(가장 많이 사용)
- `Average Pooling`: 각 작은 영역의 평균값을 계산하여 다음 계층으로 전달한다. 이 방식은 특징의 전반적인 평균 정보를 전달할 때 유용하다.

`Convolution과 Pooling을 반복적으로 수행하며 패턴을 찾고, 그 패턴을 입력데이터로 Fully-connected 신경망으로 인풋으로 사용하여 Classification을 수행한다.`

# RNN(Recurrent Neural Network: 순환 신경망)

> 시퀀스 데이터(Sequence data)나 순차적 데이터(Sequential data)를 처리하는 데 주로 사용된다. 이 네트워크는 과거의 정보를 기억하고 이를 다음 단계의 입력 입력으로 사용하여, 시간에 따라 변화하는 데이터를 학습하는 데 매우 적합하다. 주로, 자연어 처리(NLP), 음성 인식, 시계열 데이터 분석(주식 가격 예측, 날씨 예측 등), 비디오 분석에 활용 할 수 있다. RNN은 순차적 데이터의 문맥과 순서를 유지하며 학습할 수 있다는 점에서 매우 강력하지만, 긴 시퀀스에서는 기울기 소실 문제가 발생하기 쉽다.

- 입력과 출력을 시퀀스 단위로 처리하는 딥러닝 모델이다.

![1280px-Recurrent_neural_network_unfold svg](https://github.com/user-attachments/assets/9bbed6cb-d1aa-4d3f-b5c2-dc113ce68ba6)
_Image Source: [wekipedia_rnn](https://en.wikipedia.org/wiki/Recurrent_neural_network)_

> 위 이미지는 기본적인 순환신경망의 나타낸다. 위 그림을 보면 RNN은 은닉층의 노드에서 활성 함수를 통해 출력된 값이 출력층 방향으로 보내면서도 다시 은닉층 노드의 다음 계산의 입력으로 보내는 특징을 갖고 있다.(일종의 메모리 역할을 수행할 수 있다.)

## 주요 특징

- `순환 구조`: 이전 단계의 출력이 현재 단계의 입력으로 들어가는 구로를 가지며, 이를 통해 이전 단계의 정보를 현재 단계에 반영할 수 있다. 데이터의 순서와 문맥을 반영할 수 있어 시간이나 순서에 의존하는 데이터를 잘 다룰 수 있다.
- `기억 능력`: 순환 구조 덕분에 과거의 정보를 어느 정도 기억할 수 있으며, 이를 통해 시퀀스 내의 패턴을 학습한다. 다만, 긴 시퀀스에 대해서는 기억 능력이 한정적이라는 단점이 있어 이를 해결하기 위한 'Long Short-Term Memory'나 'Gated Recurrent Unit' 같은 확장 모델들이 개발되었다.

## RNN의 한계

> RNN은 긴 시퀀스를 다룰 때 기울기 소실(vanishing gradient) 문제에 취약하다. 시간이 지남에 따라 네트워크가 이전 단계의 정보를 잊어버리거나, 너무 희미하게 남겨두게 되어 장기 의존성(Long-term dependency)을 학습하기 어렵다. 이를 해결하기 위해 LSTM, GRU같은 구조가 고안되어 게이트 메커니즘을 통해 정보가 필요할 때는 저장하고, 필요하지 않으면 삭제하는 방식으로 장기 의존성 문제를 개선한다.

# CNN 과 RNN 비교

| **특징**             | **RNN (Recurrent Neural Network)**                          | **CNN (Convolutional Neural Network)**                     |
| -------------------- | ----------------------------------------------------------- | ---------------------------------------------------------- |
| **주 사용 분야**     | 시계열 데이터, 자연어 처리, 음성 인식, 비디오 분석          | 이미지 처리, 객체 인식, 이미지 분류, 영상 분석             |
| **입력 데이터 형태** | 순차적 데이터 (시퀀스 데이터), 시간에 따라 변화하는 데이터  | 2D 또는 3D 데이터 (이미지, 비디오 프레임 등)               |
| **구조 특징**        | 순환 구조로, 이전 출력이 현재 입력에 피드백                 | 컨볼루션 계층과 풀링 계층을 통한 특징 추출                 |
| **기억 메커니즘**    | 이전의 상태(은닉 상태)를 현재에 반영하여 과거 정보를 저장함 | 위치 불변 특성으로 특정 패턴의 위치에 상관없이 감지 가능   |
| **주요 연산**        | 순환 계산 (이전 상태와 현재 입력을 함께 계산)               | 컨볼루션 연산과 풀링 연산                                  |
| **장점**             | 순차적 의존성 학습 가능, 시퀀스 데이터에 적합               | 효율적인 파라미터 공유, 공간적 특징 추출에 강점            |
| **한계**             | 긴 시퀀스 학습 시 기울기 소실 문제 (해결: LSTM, GRU)        | 이미지의 시간적 순서 정보나 시퀀스 정보를 잘 반영하지 못함 |
| **주요 변형 모델**   | LSTM, GRU (장기 의존성 문제 해결)                           | ResNet, VGG, Inception 등 (더 깊은 네트워크 구성)          |
| **파라미터 수**      | 입력 길이에 따라 변동될 수 있음                             | 파라미터 공유로 인해 적음                                  |
| **데이터 처리 방식** | 시계열 데이터로 시간 단계별 처리                            | 전체 이미지 또는 부분 영역을 동시에 처리                   |
| **학습 특징**        | 각 단계마다 연속적인 학습이 가능                            | 계층이 깊어질수록 추상적인 특징을 학습할 수 있음           |
